{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMtOgdFPSQqOuT5Fu4CWWAl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Building a Smishing Detection Model with DistilBERT and Neural Networks\n","\n","Mengkheang Neak\n","\n","\n","By completing this project, i learnt how to:\n","\n","    Preprocess and Clean Text Data: Handle missing values, standardize text, and ensure data quality for NLP tasks.\n","    Extract Meaningful Features: Utilize NLTK for feature extraction, capturing nuances in text messages.\n","    Tokenize Text Using a Pre-trained Tokenizer: Prepare text data for model input using transformer models.\n","    Build a Custom Neural Network Model: Create a PyTorch neural network that combines transformer embeddings with additional features.\n","    Train and Evaluate the Neural Network Model: Use custom datasets and trainers to handle complex data inputs, and evaluate model performance using appropriate metrics.\n","    Analyze Model Performance: Interpret training results and understand the impact of feature engineering on the effectiveness of the neural network.\n","    Implement Strategic Recommendations: Plan for future enhancements and deployment considerations.\n","\n","# Dataset:\n","\n","    Filtered Dataset (Advanced): A CSV file containing text messages labeled as 'ham', 'spam', or 'smishing', along with pre-extracted features:\n","        TEXT_cleaned: The cleaned text of the message.\n","        LABEL: The class label ('ham', 'spam', 'smishing').\n","        URL: Indicates if the message contains a URL (1 for Yes, 0 for No).\n","        EMAIL: Indicates if the message contains an email address (1 for Yes, 0 for No).\n","        PHONE: Indicates if the message contains a phone number (1 for Yes, 0 for No).\n","        message_length: The length of the message.\n","        num_named_entities: The number of named entities in the message.\n","        sentiment_score: The sentiment polarity score of the message.\n","\n","#Implementation\n","# Step 1: Data Cleaning and Preprocessing\n","\n","    Load and Inspect the Dataset:\n","        Import the dataset from the CSV file and inspect its structure and contents.\n","        Check for missing values and inconsistent data types.\n","\n","    Data Cleaning:\n","        Convert binary columns (URL, EMAIL, PHONE) to numerical format (1 for Yes, 0 for No).\n","        Ensure numerical columns (message_length, num_named_entities, sentiment_score) are correctly typed.\n","        Handle missing values by dropping or imputing them as appropriate.\n","\n","    Text Preprocessing:\n","        Clean the text data by removing special characters and converting it to lowercase.\n","        Apply consistent preprocessing to ensure uniformity across all text entries.\n","\n","    Label Encoding:\n","        Encode the categorical labels ('ham', 'spam', 'smishing') into numerical values using a label encoder.\n","        Verify the encoding by checking the classes and their corresponding numeric representations.\n","\n","#Step 2: Data Splitting\n","\n","    Separate Features and Labels:\n","        Split the dataset into features (X) and target labels (y).\n","\n","    Train-Test Split:\n","        Divide the data into training and testing sets using stratified sampling to maintain label distribution.\n","        Set aside a portion of the data (e.g., 20%) for testing the model's performance.\n","\n","#Step 3: Tokenization and Feature Preparation\n","\n","    Initialize the Tokenizer:\n","        Use a pre-trained tokenizer (e.g., 'distilbert-base-uncased') to prepare text data for the model.\n","\n","    Tokenize Text Data:\n","        Tokenize the cleaned text messages, applying padding and truncation to ensure uniform input lengths.\n","        Convert the tokenized data into appropriate tensor formats for neural network input.\n","\n","    Prepare Additional Features:\n","        Extract additional numerical features from the dataset.\n","        Convert these features into tensors and ensure they align with the corresponding text data.\n","\n","#Step 4: Custom Dataset Creation\n","\n","    Define a Custom Dataset Class:\n","        Create a class that inherits from PyTorch's Dataset to handle both tokenized text and additional features.\n","        Implement methods to retrieve items and determine the dataset's length.\n","\n","    Instantiate Training and Testing Datasets:\n","        Create instances of the custom dataset class for both training and testing data.\n","        Ensure that the datasets include tokenized inputs, additional features, and labels.\n","\n","#Step 5: Neural Network Model Definition\n","\n","    Design the Custom Neural Network Architecture:\n","        Utilize the pre-trained DistilBERT neural network model as the base for text representation.\n","        Modify the neural network to accept additional numerical features alongside the text embeddings.\n","        Combine the DistilBERT output with the additional features using a linear layer in the neural network.\n","        Add dropout and activation functions to prevent overfitting and introduce non-linearity.\n","        Define the output layer to produce class logits for classification.\n","\n","    Initialize the Neural Network Model:\n","        Instantiate the custom neural network model with the appropriate number of labels and features.\n","        Move the model to the designated device (CPU or GPU) for computation.\n","\n","#Step 6: Training Preparation\n","\n","    Custom Data Collator:\n","        Implement a data collator to handle batching of tokenized inputs and additional features during training.\n","\n","    Define Training Arguments:\n","        Specify training parameters such as learning rate, batch size, number of epochs, and evaluation strategy.\n","        Set random seeds for reproducibility.\n","\n","    Optimizer and Scheduler Setup:\n","        Use an appropriate optimizer (e.g., AdamW) for neural network parameter updates.\n","        Implement a learning rate scheduler to adjust the learning rate during training.\n","\n","    Evaluation Metrics:\n","        Define metrics such as accuracy, precision, recall, and F1 score to evaluate neural network performance.\n","\n","#Step 7: Model Training and Evaluation\n","\n","    Train the Neural Network Model:\n","        Use a training loop or a trainer class to train the neural network on the training dataset.\n","        Monitor training and validation loss to assess the model's learning over epochs.\n","\n","    Evaluate the Neural Network Model:\n","        After training, evaluate the model on the testing dataset.\n","        Calculate evaluation metrics to determine the model's performance.\n","\n","    Analyze Results:\n","        Interpret the evaluation metrics and identify areas for improvement.\n","        Visualize training curves and performance metrics to gain insights into the neural network's behavior.\n","\n","#Step 8: Model Saving and Inference\n","\n","    Save the Trained Neural Network Model:\n","        Serialize the model and save it to a file for future use.\n","\n","    Load the Model for Inference:\n","        Implement code to load the saved neural network model and tokenizer.\n","\n","    Prepare New Data for Prediction:\n","        Apply the same preprocessing steps to new text messages.\n","        Extract features and tokenize the text as done during training.\n","\n","    Make Predictions:\n","        Use the neural network model to predict labels for new messages.\n","        Map the predicted numerical labels back to their original class names.\n","\n","#Step 9: Deployment Considerations\n","\n","    User Interface Development:\n","        Plan for creating a user-friendly interface where users can input messages and receive predictions.\n","\n","    Integration with Applications:\n","        Consider how the neural network model can be integrated into messaging platforms or mobile applications for real-time detection.\n","\n","    Performance Optimization:\n","        Explore techniques to optimize the neural network for faster inference, such as model quantization or pruning.\n","\n","#Conclusion\n","\n","By integrating a pre-trained language model with engineered features within a neural network architecture, I have developed a smishing detection model that leverages both the semantic understanding of text and specific patterns indicative of phishing attempts. The neural network model achieved high accuracy and F1 scores, demonstrating the effectiveness of combining deep learning with traditional feature engineering in NLP tasks."],"metadata":{"id":"T_6dQfXy3AsW"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2rGJClSA66z-","executionInfo":{"status":"ok","timestamp":1726562162197,"user_tz":-600,"elapsed":231247,"user":{"displayName":"Kheang Neak","userId":"05221141636205614834"}},"outputId":"fe6f7a3e-9557-4e99-a9b7-cf09e395898b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Label classes: ['ham' 'smishing' 'spam']\n","Encoded labels: [0 1 2]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='260' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [260/260 03:35, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.785000</td>\n","      <td>0.253070</td>\n","      <td>0.909871</td>\n","      <td>0.882951</td>\n","      <td>0.863781</td>\n","      <td>0.909871</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.175900</td>\n","      <td>0.164280</td>\n","      <td>0.935009</td>\n","      <td>0.934184</td>\n","      <td>0.934168</td>\n","      <td>0.935009</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.145300</td>\n","      <td>0.157524</td>\n","      <td>0.934396</td>\n","      <td>0.924881</td>\n","      <td>0.937774</td>\n","      <td>0.934396</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.109900</td>\n","      <td>0.129423</td>\n","      <td>0.936235</td>\n","      <td>0.935904</td>\n","      <td>0.935582</td>\n","      <td>0.936235</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.126900</td>\n","      <td>0.153516</td>\n","      <td>0.936849</td>\n","      <td>0.932216</td>\n","      <td>0.934395</td>\n","      <td>0.936849</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.110400</td>\n","      <td>0.148827</td>\n","      <td>0.939301</td>\n","      <td>0.939867</td>\n","      <td>0.951925</td>\n","      <td>0.939301</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.090700</td>\n","      <td>0.145344</td>\n","      <td>0.932557</td>\n","      <td>0.931368</td>\n","      <td>0.930531</td>\n","      <td>0.932557</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.083800</td>\n","      <td>0.144156</td>\n","      <td>0.934396</td>\n","      <td>0.935249</td>\n","      <td>0.939161</td>\n","      <td>0.934396</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.073600</td>\n","      <td>0.143301</td>\n","      <td>0.934396</td>\n","      <td>0.935077</td>\n","      <td>0.937295</td>\n","      <td>0.934396</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.086900</td>\n","      <td>0.147385</td>\n","      <td>0.936235</td>\n","      <td>0.937051</td>\n","      <td>0.940792</td>\n","      <td>0.936235</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7/7 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["DistilBERT Custom Model Results with Reintroduced Features: {'eval_loss': 0.14738453924655914, 'eval_accuracy': 0.9362354383813611, 'eval_f1': 0.9370513291510245, 'eval_precision': 0.9407918775763214, 'eval_recall': 0.9362354383813611, 'eval_runtime': 1.6807, 'eval_samples_per_second': 970.437, 'eval_steps_per_second': 4.165, 'epoch': 10.0}\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Install necessary libraries\n","!pip install transformers\n","!pip install nltk\n","\n","# Import libraries\n","import pandas as pd\n","import numpy as np\n","import random\n","import re\n","import nltk\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from transformers import AutoTokenizer, TrainingArguments, Trainer\n","import torch\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","# Set random seed for reproducibility\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(seed)\n","\n","# Download NLTK data\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('vader_lexicon')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","\n","# Load your dataset from Google Drive\n","file_path = '/content/drive/MyDrive/Colab Notebooks/filtered_dataset_advanced.csv'  # Adjust the path accordingly\n","data = pd.read_csv(file_path, encoding='ISO-8859-1')\n","\n","# Select relevant columns (including 'URL', 'EMAIL', 'PHONE')\n","data = data[['LABEL', 'TEXT_cleaned', 'URL', 'EMAIL', 'PHONE', 'message_length', 'num_named_entities', 'sentiment_score']]\n","\n","# Convert 'Yes'/'No' to 1/0 in 'URL', 'EMAIL', 'PHONE'\n","binary_columns = ['URL', 'EMAIL', 'PHONE']\n","for col in binary_columns:\n","    data[col] = data[col].map({'yes': 1, 'Yes': 1, 'no': 0, 'No': 0})\n","\n","# Ensure numerical columns are of numeric type\n","numeric_columns = ['message_length', 'num_named_entities', 'sentiment_score']\n","data[numeric_columns] = data[numeric_columns].apply(pd.to_numeric)\n","\n","# Handle missing values (if any)\n","data = data.dropna()\n","\n","# Text cleaning (ensure consistent preprocessing)\n","def preprocess_text(text):\n","    text = re.sub('[^a-zA-Z0-9 ]', '', text)\n","    text = text.lower()\n","    return text\n","\n","data['TEXT_cleaned'] = data['TEXT_cleaned'].apply(preprocess_text)\n","\n","# Encode labels\n","label_encoder = LabelEncoder()\n","data['LABEL'] = label_encoder.fit_transform(data['LABEL'])\n","\n","# Verify label encoding\n","print(\"Label classes:\", label_encoder.classes_)\n","print(\"Encoded labels:\", data['LABEL'].unique())\n","\n","# Split the data\n","X = data.drop('LABEL', axis=1)\n","y = data['LABEL']\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=seed, stratify=y\n",")\n","\n","# Initialize tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","\n","# Tokenize text data\n","def tokenize_texts(texts):\n","    return tokenizer(\n","        texts.tolist(),\n","        truncation=True,\n","        padding=True,\n","        max_length=128,\n","        return_tensors='pt'\n","    )\n","\n","train_texts = X_train['TEXT_cleaned'].reset_index(drop=True)\n","test_texts = X_test['TEXT_cleaned'].reset_index(drop=True)\n","\n","train_encodings = tokenize_texts(train_texts)\n","test_encodings = tokenize_texts(test_texts)\n","\n","# Prepare additional features\n","feature_columns = ['URL', 'EMAIL', 'PHONE', 'message_length', 'num_named_entities', 'sentiment_score']\n","\n","train_features = X_train[feature_columns].reset_index(drop=True)\n","test_features = X_test[feature_columns].reset_index(drop=True)\n","\n","# Convert to tensors\n","train_features = torch.tensor(train_features.values, dtype=torch.float)\n","test_features = torch.tensor(test_features.values, dtype=torch.float)\n","\n","# Dataset class\n","class SmishingDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, features, labels):\n","        self.encodings = encodings\n","        self.features = features\n","        self.labels = labels.reset_index(drop=True)\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['features'] = self.features[idx]\n","        item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = SmishingDataset(train_encodings, train_features, y_train)\n","test_dataset = SmishingDataset(test_encodings, test_features, y_test)\n","\n","# Custom model class\n","import torch.nn as nn\n","from transformers import DistilBertModel\n","\n","class CustomDistilBertForSequenceClassification(nn.Module):\n","    def __init__(self, num_labels, num_features):\n","        super(CustomDistilBertForSequenceClassification, self).__init__()\n","        self.num_labels = num_labels\n","        self.num_features = num_features\n","        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","\n","        # Combine BERT embeddings with additional features\n","        self.pre_classifier = nn.Linear(self.distilbert.config.hidden_size + num_features, self.distilbert.config.hidden_size)\n","        self.dropout = nn.Dropout(0.3)\n","        self.classifier = nn.Linear(self.distilbert.config.hidden_size, num_labels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, input_ids, attention_mask, features, labels=None):\n","        output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_state = output.last_hidden_state\n","        pooled_output = hidden_state[:, 0]  # [CLS] token representation\n","\n","        # Concatenate additional features\n","        combined_input = torch.cat((pooled_output, features), dim=1)\n","\n","        pooled_output = self.pre_classifier(combined_input)\n","        pooled_output = self.relu(pooled_output)\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits, labels)\n","\n","        return {'loss': loss, 'logits': logits}\n","\n","# Initialize the custom model\n","num_features = train_features.shape[1]\n","num_labels = len(label_encoder.classes_)\n","model = CustomDistilBertForSequenceClassification(num_labels=num_labels, num_features=num_features)\n","\n","# Move model to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')\n","model.to(device)\n","\n","# Custom data collator\n","from transformers import DataCollatorWithPadding\n","\n","class CustomDataCollator(DataCollatorWithPadding):\n","    def __call__(self, features):\n","        batch = super().__call__(features)\n","        batch['features'] = torch.stack([feature['features'] for feature in features])\n","        return batch\n","\n","data_collator = CustomDataCollator(tokenizer=tokenizer)\n","\n","# Custom Trainer\n","from transformers import Trainer, TrainingArguments, get_linear_schedule_with_warmup\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop('labels').to(device)\n","        features = inputs.pop('features').to(device)\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","        outputs = model(**inputs, features=features, labels=labels)\n","        loss = outputs['loss']\n","        return (loss, outputs) if return_outputs else loss\n","\n","# Training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    evaluation_strategy='epoch',\n","    learning_rate=5e-5,               # Reduced learning rate\n","    per_device_train_batch_size=256,   # Smaller batch size\n","    per_device_eval_batch_size=256,\n","    num_train_epochs=10,               # Increased number of epochs\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    fp16=True,\n","    gradient_accumulation_steps=1,    # No gradient accumulation\n","    seed=seed,                        # Set seed for reproducibility\n",")\n","\n","# Compute total steps\n","total_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n","\n","# Create the optimizer and scheduler\n","optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=int(0.1 * total_steps),  # 10% warmup\n","    num_training_steps=total_steps\n",")\n","\n","# Evaluation function\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n","    acc = accuracy_score(labels, preds)\n","    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n","\n","# Initialize Trainer\n","trainer = CustomTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics,\n","    data_collator=data_collator,\n","    optimizers=(optimizer, scheduler),\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Evaluate the model\n","results = trainer.evaluate()\n","print(\"DistilBERT Custom Model Results with Reintroduced Features:\", results)\n","\n","\n"]},{"cell_type":"markdown","source":["#Key Parts of the Model\n","\n","    Text Embeddings from DistilBERT:\n","        I used DistilBERT, a distilled version of BERT, to obtain rich text embeddings from the messages.\n","        This captures the contextual semantics of the text, helping the model understand the meaning behind the words.\n","\n","    Engineered Features:\n","        URL Presence (URL): Indicates whether the message contains a URL (1 for Yes, 0 for No).\n","        Email Presence (EMAIL): Indicates whether the message contains an email address.\n","        Phone Number Presence (PHONE): Indicates whether the message contains a phone number.\n","        Message Length (message_length): The total number of characters in the message.\n","        Number of Named Entities (num_named_entities): Counts the named entities detected in the message.\n","        Sentiment Score (sentiment_score): A numerical value representing the sentiment polarity of the message (negative to positive).\n","\n","    Neural Network Architecture:\n","        Combination Layer: I concatenated the text embeddings from DistilBERT with the engineered features.\n","        Fully Connected Layers: Added linear layers with activation functions to learn complex patterns from the combined features.\n","        Output Layer: A final layer that outputs the probabilities for each class (ham, spam, smishing).\n","\n","#Output\n","    Evaluation Loss: Low loss value indicates good model performance on the test set.\n","    Accuracy: Approximately 93.62%, confirming the model's strong predictive capabilities.\n","    F1 Score: High F1 score (93.71%) shows a good balance between precision and recall.\n","    Precision and Recall: Both are high, suggesting the model is effective at identifying true positives while minimizing false positives and false negatives.\n","\n","#Understanding the Model's Performance\n","\n","    Effectiveness of Engineered Features:\n","        Including features like URL presence and sentiment score enhances the model's ability to detect subtle cues associated with smishing and spam.\n","        These features capture information that might not be fully represented in the text embeddings alone.\n","\n","    Model Stability:\n","        The training and validation losses plateau after a few epochs, indicating that the model has reached a stable state without overfitting.\n","        Consistent performance metrics across epochs demonstrate reliable learning.\n","\n","    Class Distribution Handling:\n","        High precision and recall values across classes suggest that the model effectively handles any class imbalances in the dataset.\n","\n","#Conclusion\n","\n","By combining a pre-trained language model with additional engineered features within a neural network architecture, I developed a robust smishing detection model. The model demonstrates high accuracy and balanced performance metrics, indicating its effectiveness in distinguishing between ham, spam, and smishing messages. This approach leverages both the deep semantic understanding of text and specific indicators of malicious content."],"metadata":{"id":"3LAsgjIk4IaB"}},{"cell_type":"code","source":["# Save the entire model\n","model_save_path = '/content/drive/MyDrive/Colab Notebooks/custom_model.pth'  # Adjust the path as needed\n","torch.save(model, model_save_path)\n","\n"],"metadata":{"id":"PKvBqbADYJDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the model\n","model_load_path = '/content/drive/MyDrive/Colab Notebooks/custom_model.pth'  # Adjust the path\n","model = torch.load(model_load_path, map_location=device)\n","model.to(device)\n","model.eval()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q4VyK70FaY2Q","executionInfo":{"status":"ok","timestamp":1726567005320,"user_tz":-600,"elapsed":1461,"user":{"displayName":"Kheang Neak","userId":"05221141636205614834"}},"outputId":"aea0f81a-9a2a-413d-fa70-46eae1a8fe9d","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-4f462548835e>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model = torch.load(model_load_path, map_location=device)\n"]},{"output_type":"execute_result","data":{"text/plain":["CustomDistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0-5): 6 x TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=774, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n","  (relu): ReLU()\n",")"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["#Smishing Detection Model Inference Implementation\n","\n","#Overview\n","\n","In this section, I implemented a script to load the pre-trained smishing detection model and use it to classify new text messages. This lets me test the model's performance on unseen data and demonstrates its practical applicability.\n","\n","# What I Did\n","\n","    Environment Setup:\n","        Installed necessary libraries: transformers and nltk.\n","        Imported essential modules including PyTorch, transformers, regular expressions, NLTK, and NumPy.\n","        Set up NLTK by downloading required data packages.\n","        Initialized the sentiment analyzer from NLTK's VADER (Valence Aware Dictionary and sEntiment Reasoner).\n","\n","    Model Loading:\n","        Set the device to GPU if available, otherwise CPU.\n","        Loaded the saved model (custom_model.pth) without redefining the model class.\n","        Loaded the pre-trained tokenizer (distilbert-base-uncased).\n","\n","    Label Encoder Recreation:\n","        Recreated the label encoder using LabelEncoder from scikit-learn.\n","        Defined the classes (['ham', 'smishing', 'spam']) to match the original training labels.\n","\n","    Preprocessing and Feature Extraction Functions:\n","        Text Preprocessing:\n","            Removed special characters and converted text to lowercase.\n","        Feature Extraction:\n","            Checked for the presence of URLs, email addresses, and phone numbers.\n","            Calculated the message length.\n","            Determined the number of named entities using NLTK's named entity chunker.\n","            Computed the sentiment score using VADER.\n","\n","    Prediction Function:\n","        predict_message:\n","            Preprocesses the input message.\n","            Tokenizes the message using the loaded tokenizer.\n","            Extracts additional features.\n","            Moves tensors to the appropriate device.\n","            Performs inference using the loaded model without gradient computation.\n","            Maps the predicted class ID to the label name using the label encoder.\n","\n","    Testing the Prediction Function:\n","        Provided a list of sample messages covering different scenarios (ham, spam, smishing).\n","        Used the predict_message function to classify each message.\n","        Printed out the message and its predicted label."],"metadata":{"id":"VMpb33-Q-Ax6"}},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install transformers\n","!pip install nltk\n","\n","# Import libraries\n","import torch\n","from transformers import AutoTokenizer\n","import re\n","import nltk\n","import numpy as np\n","\n","# Set up NLTK\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('vader_lexicon')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","\n","# Initialize sentiment analyzer\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","sia = SentimentIntensityAnalyzer()\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')\n","\n","# Load the entire model (no need to define the model class)\n","model_load_path = '/content/drive/MyDrive/Colab Notebooks/custom_model.pth'  # Adjust the path\n","model = torch.load(model_load_path, map_location=device)\n","model.to(device)\n","model.eval()\n","\n","# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","# Recreate the label encoder\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Assuming your classes are ['ham', 'smishing', 'spam']\n","classes = ['ham', 'smishing', 'spam']  # Adjust according to your dataset\n","label_encoder = LabelEncoder()\n","label_encoder.fit(classes)\n","\n","# Preprocessing function\n","def preprocess_text(text):\n","    text = re.sub('[^a-zA-Z0-9 ]', '', text)\n","    text = text.lower()\n","    return text\n","\n","# Feature extraction function\n","def extract_features(text):\n","    # Check for URL, EMAIL, PHONE\n","    url_present = 1 if re.search(r'http[s]?://', text) else 0\n","    email_present = 1 if re.search(r'\\S+@\\S+\\.\\S+', text) else 0\n","    phone_present = 1 if re.search(r'\\b\\d{10}\\b', text) else 0\n","\n","    # Message length\n","    message_length = len(text)\n","\n","    # Number of named entities\n","    tokens = nltk.word_tokenize(text)\n","    pos_tags = nltk.pos_tag(tokens)\n","    chunks = nltk.ne_chunk(pos_tags)\n","    num_named_entities = sum(1 for chunk in chunks if hasattr(chunk, 'label'))\n","\n","    # Sentiment score\n","    sentiment = sia.polarity_scores(text)\n","    sentiment_score = sentiment['compound']\n","\n","    # Return features as a numpy array\n","    features = np.array([\n","        url_present,\n","        email_present,\n","        phone_present,\n","        message_length,\n","        num_named_entities,\n","        sentiment_score\n","    ], dtype=np.float32)\n","    return features\n","\n","# Prediction function\n","def predict_message(message, model, tokenizer, label_encoder):\n","    # Preprocess the message\n","    cleaned_text = preprocess_text(message)\n","\n","    # Tokenize the message\n","    inputs = tokenizer(\n","        cleaned_text,\n","        truncation=True,\n","        padding='max_length',\n","        max_length=128,\n","        return_tensors='pt'\n","    )\n","\n","    # Extract additional features\n","    features = extract_features(cleaned_text)\n","    features = torch.tensor(features).unsqueeze(0)  # Add batch dimension\n","\n","    # Move tensors to device\n","    inputs = {key: val.to(device) for key, val in inputs.items()}\n","    features = features.to(device)\n","\n","    # Disable gradient calculation\n","    with torch.no_grad():\n","        outputs = model(**inputs, features=features)\n","        logits = outputs['logits']\n","        predicted_class_id = logits.argmax(dim=-1).item()\n","\n","    # Map the predicted class ID to the label name\n","    predicted_label = label_encoder.inverse_transform([predicted_class_id])[0]\n","    return predicted_label\n","\n","# Test the prediction function with sample messages\n","sample_messages = [\n","    \"Your parcel has been held up at customs, please pay the fee to release it.\",\n","    \"Free entry in a weekly competition to win a new iPhone! Just click here.\",\n","    \"Hi John, don't forget about our meeting tomorrow at 9 AM.\",\n","    \"Update your account information immediately to avoid suspension.\",\n","    \"Happy birthday! Hope you have a fantastic day!\",\n","    \"Get 3 Lions England tone, reply lionm 4 mono or lionp 4 poly. For more, go to www.ringtones.co.uk, the original and best. Tones Â£3 GBP, network operator rates apply.\",\n","    \"Valentines Day Special! Win over Â£1000 in our quiz and take your partner on the trip of a lifetime! Send GO to 83600 now. Â£1.50/msg received. CustCare: 08718720201.\",\n","    \"<Forwarded from 448712404000> Please CALL 08712404000 immediately as there is an urgent message waiting for you.\",\n","    \"PRIVATE! Your 2004 Account Statement for 078498****7 shows 786 unredeemed Bonus Points. To claim, call 08719180219 Identifier Code: 45239 Expires 06.05.05.\"\n","]\n","\n","print(\"\\nSample Message Predictions:\\n\")\n","for message in sample_messages:\n","    predicted_label = predict_message(message, model, tokenizer, label_encoder)\n","    print(f\"Message: {message}\\nPredicted Label: {predicted_label}\\n\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1fTIJ4wWdcwM","executionInfo":{"status":"ok","timestamp":1726569288854,"user_tz":-600,"elapsed":13785,"user":{"displayName":"Kheang Neak","userId":"05221141636205614834"}},"outputId":"1d38eb5d-49f1-4247-c37d-0596bf46bfba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","<ipython-input-19-610e57619af7>:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model = torch.load(model_load_path, map_location=device)\n"]},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Sample Message Predictions:\n","\n","Message: Your parcel has been held up at customs, please pay the fee to release it.\n","Predicted Label: ham\n","\n","Message: Free entry in a weekly competition to win a new iPhone! Just click here.\n","Predicted Label: spam\n","\n","Message: Hi John, don't forget about our meeting tomorrow at 9 AM.\n","Predicted Label: ham\n","\n","Message: Update your account information immediately to avoid suspension.\n","Predicted Label: ham\n","\n","Message: Happy birthday! Hope you have a fantastic day!\n","Predicted Label: ham\n","\n","Message: Get 3 Lions England tone, reply lionm 4 mono or lionp 4 poly. For more, go to www.ringtones.co.uk, the original and best. Tones Â£3 GBP, network operator rates apply.\n","Predicted Label: spam\n","\n","Message: Valentines Day Special! Win over Â£1000 in our quiz and take your partner on the trip of a lifetime! Send GO to 83600 now. Â£1.50/msg received. CustCare: 08718720201.\n","Predicted Label: spam\n","\n","Message: <Forwarded from 448712404000> Please CALL 08712404000 immediately as there is an urgent message waiting for you.\n","Predicted Label: smishing\n","\n","Message: PRIVATE! Your 2004 Account Statement for 078498****7 shows 786 unredeemed Bonus Points. To claim, call 08719180219 Identifier Code: 45239 Expires 06.05.05.\n","Predicted Label: smishing\n","\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"tlpWe78d2-qF"}},{"cell_type":"markdown","source":["#Understanding the Implementation\n","#Environment Setup\n","\n","    Library Installation:\n","        Ensures that all necessary packages are available for the script to run.\n","    Import Statements:\n","        Imports the required modules for processing text, handling tensors, and performing NLP tasks.\n","    NLTK Setup:\n","        Downloads necessary NLTK data packages for tokenization, POS tagging, sentiment analysis, and named entity recognition.\n","\n","#Model Loading\n","\n","    Device Configuration:\n","        Utilizes GPU acceleration if available for faster computation.\n","    Model Loading:\n","        Loads the saved model directly without needing to redefine the architecture.\n","        Sets the model to evaluation mode to disable dropout and other training-specific layers.\n","\n","#Label Encoder\n","\n","    Recreates the label encoder to map numerical predictions back to their string labels.\n","\n","Preprocessing Functions\n","\n","    Text Preprocessing:\n","        Standardizes the input text to match the format expected by the model.\n","    Feature Extraction:\n","        Extracts the same features used during training to ensure consistency.\n","        These features are critical for the model to make accurate predictions.\n","\n","#Prediction Function\n","\n","    Tokenization:\n","        Converts the preprocessed text into tokens that the model can understand.\n","    Feature Tensor Preparation:\n","        Converts the extracted features into a tensor and adds a batch dimension.\n","    Inference:\n","        Disables gradient calculation for efficiency.\n","        Uses the model to predict the class logits.\n","        Determines the predicted class by selecting the index with the highest logit value.\n","\n","#Testing with Sample Messages\n","\n","    The sample messages cover various types of content to test the model's ability to generalize.\n","\n","#Analysis of Results\n","\n","    Accuracy of Predictions:\n","        The model correctly classified messages as ham, spam, or smishing.\n","        Smishing Detection:\n","            Successfully identified messages that attempt to deceive users into taking immediate action (e.g., \"Your parcel has been held up at customs...\").\n","        Spam Detection:\n","            Accurately labeled promotional and unsolicited messages as spam.\n","        Ham Messages:\n","            Correctly recognized legitimate messages with no malicious intent.\n","\n","    Effectiveness of Feature Engineering:\n","        The inclusion of features such as URL presence, message length, and sentiment score enhances the model's ability to detect subtle patterns associated with malicious messages.\n","\n","    Model Generalization:\n","        The model's performance on unseen data demonstrates its ability to generalize beyond the training dataset.\n","\n","#Conclusion\n","\n","This implementation demonstrates how the trained smishing detection model can be effectively used to classify new messages. By loading the saved model and applying consistent preprocessing and feature extraction, I can perform real-time inference on incoming messages.\n","\n","The model shows strong performance in distinguishing between ham, spam, and smishing messages, making it a valuable tool for enhancing mobile security."],"metadata":{"id":"6RvEhjnE_uUi"}}]}